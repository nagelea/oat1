#!/usr/bin/env python3
"""
æ‰«æå†å²æ›´æ–°è„šæœ¬
æ›´æ–°æ‰«æå†å²è®°å½•ï¼Œç”¨äºä¸‹æ¬¡æ¯”è¾ƒ
"""

import json
import os
import hashlib
from datetime import datetime


def calculate_results_hash(results_file='reports/raw_data.json'):
    """è®¡ç®—ç»“æœçš„å“ˆå¸Œå€¼"""
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–å…³é”®ä¿¡æ¯ç”¨äºå“ˆå¸Œè®¡ç®—
        key_data = []
        for result in data.get('results', []):
            key_info = {
                'repo': result['repository']['full_name'],
                'file': result['file']['path'],
                'matches': result['file']['match_count'],
                'last_modified': result.get('time_info', {}).get('last_commit', {}).get('last_modified', '')
            }
            key_data.append(key_info)
        
        # æŒ‰ä»“åº“å’Œæ–‡ä»¶è·¯å¾„æ’åºä»¥ç¡®ä¿ä¸€è‡´æ€§
        key_data.sort(key=lambda x: (x['repo'], x['file']))
        
        # è®¡ç®—å“ˆå¸Œ
        content = json.dumps(key_data, sort_keys=True).encode()
        return hashlib.sha256(content).hexdigest()
        
    except Exception as e:
        print(f"âŒ è®¡ç®—å“ˆå¸Œå¤±è´¥: {e}")
        return None


def update_scan_history():
    """æ›´æ–°æ‰«æå†å²"""
    try:
        # ç¡®ä¿å†å²ç›®å½•å­˜åœ¨
        history_dir = '.github/scan_history'
        os.makedirs(history_dir, exist_ok=True)
        
        # è®¡ç®—å½“å‰ç»“æœå“ˆå¸Œ
        current_hash = calculate_results_hash()
        if not current_hash:
            print("âŒ æ— æ³•è®¡ç®—å½“å‰ç»“æœå“ˆå¸Œ")
            return False
        
        # æ›´æ–°å“ˆå¸Œæ–‡ä»¶
        hash_file = os.path.join(history_dir, 'last_scan_hash.txt')
        with open(hash_file, 'w') as f:
            f.write(current_hash)
        
        print(f"âœ… æ‰«æå“ˆå¸Œå·²æ›´æ–°: {current_hash[:12]}...")
        
        # åŠ è½½å½“å‰æ‰«ææ•°æ®
        try:
            with open('reports/raw_data.json', 'r', encoding='utf-8') as f:
                current_data = json.load(f)
        except:
            current_data = {}
        
        # åˆ›å»ºå†å²è®°å½•æ¡ç›®
        history_entry = {
            'scan_time': current_data.get('scan_time', datetime.now().isoformat()),
            'hash': current_hash,
            'total_found': current_data.get('total_found', 0),
            'analyzed_files': current_data.get('analyzed_files', 0),
            'search_pattern': current_data.get('search_pattern', ''),
            'search_scope': current_data.get('search_scope', ''),
            'file_extensions': current_data.get('file_extensions', []),
            'summary': create_scan_summary(current_data)
        }
        
        # æ›´æ–°å†å²è®°å½•æ–‡ä»¶
        history_file = os.path.join(history_dir, 'scan_history.json')
        history_data = load_scan_history(history_file)
        
        # æ·»åŠ æ–°è®°å½•åˆ°å†å²
        history_data['scans'].append(history_entry)
        
        # åªä¿ç•™æœ€è¿‘50æ¬¡æ‰«æè®°å½•
        if len(history_data['scans']) > 50:
            history_data['scans'] = history_data['scans'][-50:]
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        history_data['last_updated'] = datetime.now().isoformat()
        history_data['total_scans'] = len(history_data['scans'])
        
        # ä¿å­˜å†å²è®°å½•
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š æ‰«æå†å²å·²æ›´æ–°ï¼Œæ€»è®°å½•æ•°: {history_data['total_scans']}")
        
        # ç”Ÿæˆæ‰«æè¶‹åŠ¿æŠ¥å‘Š
        generate_trend_report(history_data, history_dir)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ›´æ–°æ‰«æå†å²å¤±è´¥: {e}")
        return False


def load_scan_history(history_file):
    """åŠ è½½æ‰«æå†å²"""
    try:
        with open(history_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        # é¦–æ¬¡è¿è¡Œï¼Œåˆ›å»ºæ–°çš„å†å²ç»“æ„
        return {
            'created_at': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat(),
            'total_scans': 0,
            'scans': []
        }
    except Exception as e:
        print(f"âš ï¸ åŠ è½½å†å²è®°å½•å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°çš„å†å²è®°å½•")
        return {
            'created_at': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat(),
            'total_scans': 0,
            'scans': []
        }


def create_scan_summary(current_data):
    """åˆ›å»ºæ‰«ææ‘˜è¦"""
    results = current_data.get('results', [])
    
    if not results:
        return {
            'total_files': 0,
            'public_files': 0,
            'private_files': 0,
            'total_matches': 0,
            'repositories': [],
            'risk_level': 'NONE'
        }
    
    public_files = sum(1 for r in results if not r['repository']['private'])
    private_files = len(results) - public_files
    total_matches = sum(r['file']['match_count'] for r in results)
    repositories = list(set(r['repository']['full_name'] for r in results))
    
    # è®¡ç®—é£é™©ç­‰çº§
    if public_files > 0:
        if total_matches > 10:
            risk_level = "CRITICAL"
        elif total_matches > 5:
            risk_level = "HIGH"
        else:
            risk_level = "MEDIUM"
    elif len(results) > 0:
        risk_level = "LOW"
    else:
        risk_level = "NONE"
    
    return {
        'total_files': len(results),
        'public_files': public_files,
        'private_files': private_files,
        'total_matches': total_matches,
        'repositories': repositories,
        'repository_count': len(repositories),
        'risk_level': risk_level
    }


def generate_trend_report(history_data, history_dir):
    """ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š"""
    try:
        scans = history_data.get('scans', [])
        if len(scans) < 2:
            print("ğŸ“ˆ æ‰«æè®°å½•ä¸è¶³ï¼Œè·³è¿‡è¶‹åŠ¿åˆ†æ")
            return
        
        # æœ€è¿‘7å¤©çš„æ•°æ®
        recent_scans = scans[-7:] if len(scans) >= 7 else scans
        
        # è®¡ç®—è¶‹åŠ¿
        trend_data = {
            'period': f"æœ€è¿‘ {len(recent_scans)} æ¬¡æ‰«æ",
            'first_scan': recent_scans[0]['scan_time'],
            'last_scan': recent_scans[-1]['scan_time'],
            'trends': {
                'total_files': [scan['summary']['total_files'] for scan in recent_scans],
                'public_files': [scan['summary']['public_files'] for scan in recent_scans],
                'total_matches': [scan['summary']['total_matches'] for scan in recent_scans],
                'risk_levels': [scan['summary']['risk_level'] for scan in recent_scans]
            },
            'statistics': {}
        }
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_files_trend = trend_data['trends']['total_files']
        public_files_trend = trend_data['trends']['public_files']
        matches_trend = trend_data['trends']['total_matches']
        
        if len(total_files_trend) >= 2:
            trend_data['statistics'] = {
                'avg_total_files': sum(total_files_trend) / len(total_files_trend),
                'max_total_files': max(total_files_trend),
                'min_total_files': min(total_files_trend),
                'avg_public_files': sum(public_files_trend) / len(public_files_trend),
                'max_public_files': max(public_files_trend),
                'avg_matches': sum(matches_trend) / len(matches_trend),
                'max_matches': max(matches_trend),
                'latest_vs_previous': {
                    'total_files_change': total_files_trend[-1] - total_files_trend[-2],
                    'public_files_change': public_files_trend[-1] - public_files_trend[-2],
                    'matches_change': matches_trend[-1] - matches_trend[-2]
                }
            }
        
        # é£é™©ç­‰çº§åˆ†å¸ƒ
        risk_distribution = {}
        for risk in trend_data['trends']['risk_levels']:
            risk_distribution[risk] = risk_distribution.get(risk, 0) + 1
        
        trend_data['risk_distribution'] = risk_distribution
        
        # ä¿å­˜è¶‹åŠ¿æŠ¥å‘Š
        trend_file = os.path.join(history_dir, 'trend_report.json')
        with open(trend_file, 'w', encoding='utf-8') as f:
            json.dump(trend_data, f, indent=2, ensure_ascii=False)
        
        print("ğŸ“ˆ è¶‹åŠ¿æŠ¥å‘Šå·²ç”Ÿæˆ")
        
        # æ‰“å°ç®€è¦è¶‹åŠ¿ä¿¡æ¯
        if trend_data['statistics']:
            stats = trend_data['statistics']
            latest_change = stats['latest_vs_previous']
            
            print(f"ğŸ“Š è¶‹åŠ¿æ‘˜è¦:")
            print(f"  â€¢ å¹³å‡æ–‡ä»¶æ•°: {stats['avg_total_files']:.1f}")
            print(f"  â€¢ å¹³å‡å…¬å¼€æ–‡ä»¶: {stats['avg_public_files']:.1f}")
            print(f"  â€¢ å¹³å‡åŒ¹é…æ•°: {stats['avg_matches']:.1f}")
            
            if latest_change['total_files_change'] != 0:
                change_symbol = "ğŸ“ˆ" if latest_change['total_files_change'] > 0 else "ğŸ“‰"
                print(f"  â€¢ æ–‡ä»¶æ•°å˜åŒ–: {change_symbol} {latest_change['total_files_change']:+d}")
            
            if latest_change['public_files_change'] != 0:
                change_symbol = "âš ï¸" if latest_change['public_files_change'] > 0 else "âœ…"
                print(f"  â€¢ å…¬å¼€æ–‡ä»¶å˜åŒ–: {change_symbol} {latest_change['public_files_change']:+d}")
        
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Šå¤±è´¥: {e}")


def cleanup_old_files(history_dir):
    """æ¸…ç†æ—§æ–‡ä»¶"""
    try:
        # æ¸…ç†è¶…è¿‡30å¤©çš„ä¸´æ—¶æ–‡ä»¶
        import glob
        import time
        
        temp_pattern = os.path.join(history_dir, '*.tmp')
        current_time = time.time()
        
        for temp_file in glob.glob(temp_pattern):
            file_age = current_time - os.path.getmtime(temp_file)
            if file_age > 30 * 24 * 3600:  # 30å¤©
                os.remove(temp_file)
                print(f"ğŸ—‘ï¸ æ¸…ç†æ—§æ–‡ä»¶: {temp_file}")
                
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†æ—§æ–‡ä»¶å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸ“ æ›´æ–°æ‰«æå†å²...")
        
        success = update_scan_history()
        
        if success:
            # æ¸…ç†æ—§æ–‡ä»¶
            cleanup_old_files('.github/scan_history')
            print("âœ… æ‰«æå†å²æ›´æ–°å®Œæˆ")
        else:
            print("âŒ æ‰«æå†å²æ›´æ–°å¤±è´¥")
            exit(1)
            
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        exit(1)


if __name__ == "__main__":
    main()
