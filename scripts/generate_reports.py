#!/usr/bin/env python3
"""
æŠ¥å‘Šç”Ÿæˆè„šæœ¬
ç”Ÿæˆå¤šç§æ ¼å¼çš„å®‰å…¨æ‰«ææŠ¥å‘Š
"""

import json
import os
from datetime import datetime
from typing import Dict, List
import csv


class ReportGenerator:
    def __init__(self):
        self.raw_data_file = 'reports/raw_data.json'
        self.reports_dir = 'reports'
        
        # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # åŠ è½½åŸå§‹æ•°æ®
        self.data = self._load_raw_data()
    
    def _load_raw_data(self) -> Dict:
        """åŠ è½½åŸå§‹æ•°æ®"""
        try:
            with open(self.raw_data_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print("âŒ åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæœç´¢è„šæœ¬")
            exit(1)
        except Exception as e:
            print(f"âŒ åŠ è½½åŸå§‹æ•°æ®å¤±è´¥: {e}")
            exit(1)
    
    def generate_all_reports(self):
        """ç”Ÿæˆæ‰€æœ‰æ ¼å¼çš„æŠ¥å‘Š"""
        print("ğŸ“ å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")
        
        self.generate_text_report()
        self.generate_detailed_json_report()
        self.generate_csv_report()
        self.generate_markdown_report()
        self.generate_summary_report()
        
        print("âœ… æ‰€æœ‰æŠ¥å‘Šå·²ç”Ÿæˆå®Œæˆï¼")
    
    def generate_text_report(self):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        print("ğŸ“„ ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š...")
        
        results = self.data.get('results', [])
        total_found = self.data.get('total_found', 0)
        
        with open(f'{self.reports_dir}/search_results.txt', 'w', encoding='utf-8') as f:
            f.write(f"GitHub ä»£ç æœç´¢ç»“æœæŠ¥å‘Š\n")
            f.write("=" * 80 + "\n")
            f.write(f"æ‰«ææ—¶é—´: {self.data.get('scan_time', 'N/A')}\n")
            f.write(f"æœç´¢æŸ¥è¯¢: {self.data.get('search_query', 'N/A')}\n")
            f.write(f"æœç´¢æ¨¡å¼: {self.data.get('search_pattern', 'N/A')}\n")
            f.write(f"æ–‡ä»¶ç±»å‹: {', '.join(self.data.get('file_extensions', []))}\n")
            f.write(f"æœç´¢èŒƒå›´: {self.data.get('search_scope', 'å…¨éƒ¨å…¬å¼€ä»“åº“')}\n")
            f.write(f"æ€»å…±æ‰¾åˆ°: {total_found} ä¸ªç»“æœ\n")
            f.write(f"å·²åˆ†æ: {len(results)} ä¸ªæ–‡ä»¶\n\n")
            
            if not results:
                f.write("âœ… æœªå‘ç°åŒ…å«æ•æ„Ÿå†…å®¹çš„æ–‡ä»¶\n")
                return
            
            f.write("âš ï¸  å‘ç°ä»¥ä¸‹æ–‡ä»¶åŒ…å«æ•æ„Ÿå†…å®¹:\n\n")
            
            for i, result in enumerate(results, 1):
                self._write_file_details(f, i, result)
    
    def _write_file_details(self, f, index: int, result: Dict):
        """å†™å…¥æ–‡ä»¶è¯¦ç»†ä¿¡æ¯"""
        repo = result['repository']
        file_info = result['file']
        time_info = result.get('time_info', {})
        first_commit = time_info.get('first_commit', {})
        last_commit = time_info.get('last_commit', {})
        change_info = result.get('change_info', {})
        
        f.write(f"{index}. ä»“åº“: {repo['full_name']}\n")
        f.write(f"   æ–‡ä»¶: {file_info['path']}\n")
        f.write(f"   URL: {file_info['html_url']}\n")
        f.write(f"   ä»“åº“ç±»å‹: {'ç§æœ‰' if repo['private'] else 'å…¬å¼€'}\n")
        f.write(f"   æ–‡ä»¶å¤§å°: {file_info['size']} bytes\n")
        f.write(f"   åŒ¹é…æ¬¡æ•°: {file_info['match_count']}\n")
        
        # è¯¦ç»†æ—¶é—´ä¿¡æ¯
        f.write(f"\n   ğŸ“… æ—¶é—´ä¿¡æ¯:\n")
        if first_commit:
            f.write(f"   â”œâ”€ é¦–æ¬¡åˆ›å»º: {first_commit.get('first_created', 'N/A')}\n")
            f.write(f"   â”œâ”€ åˆ›å»ºä½œè€…: {first_commit.get('first_author', 'N/A')}\n")
            f.write(f"   â”œâ”€ åˆ›å»ºæäº¤: {first_commit.get('first_commit_sha', 'N/A')[:8]}\n")
            f.write(f"   â”œâ”€ åˆ›å»ºæ¶ˆæ¯: {first_commit.get('first_commit_message', 'N/A')}\n")
        
        if last_commit:
            f.write(f"   â”œâ”€ æœ€åä¿®æ”¹: {last_commit.get('last_modified', 'N/A')}\n")
            f.write(f"   â”œâ”€ ä¿®æ”¹ä½œè€…: {last_commit.get('last_author', 'N/A')}\n")
            f.write(f"   â”œâ”€ æœ€æ–°æäº¤: {last_commit.get('last_commit_sha', 'N/A')[:8]}\n")
            f.write(f"   â”œâ”€ æäº¤æ¶ˆæ¯: {last_commit.get('last_commit_message', 'N/A')}\n")
        
        file_age = time_info.get('file_age_days')
        if file_age is not None:
            f.write(f"   â”œâ”€ æ–‡ä»¶å¹´é¾„: {file_age} å¤©\n")
        
        total_commits = time_info.get('total_commits', 0)
        f.write(f"   â””â”€ æ€»æäº¤æ•°: {total_commits}\n")
        
        # å˜æ›´ä¿¡æ¯
        if change_info:
            f.write(f"\n   ğŸ“Š æœ€è¿‘å˜æ›´:\n")
            f.write(f"   â”œâ”€ çŠ¶æ€: {change_info.get('status', 'N/A')}\n")
            f.write(f"   â”œâ”€ æ–°å¢è¡Œæ•°: {change_info.get('additions', 0)}\n")
            f.write(f"   â”œâ”€ åˆ é™¤è¡Œæ•°: {change_info.get('deletions', 0)}\n")
            f.write(f"   â””â”€ æ€»å˜æ›´è¡Œæ•°: {change_info.get('changes', 0)}\n")
            if change_info.get('previous_filename'):
                f.write(f"   â””â”€ åŸæ–‡ä»¶å: {change_info['previous_filename']}\n")
        
        # ä¿®æ”¹å†å²
        commit_history = time_info.get('commit_history', [])
        if commit_history:
            f.write(f"\n   ğŸ“ æœ€è¿‘ä¿®æ”¹å†å²:\n")
            for j, commit in enumerate(commit_history[:5], 1):
                f.write(f"   {j}. {commit['date'][:10]} - {commit['author']} - {commit['message']}\n")
        
        f.write(f"\n   ğŸ¢ ä»“åº“ä¿¡æ¯:\n")
        f.write(f"   â”œâ”€ æè¿°: {repo['description']}\n")
        f.write(f"   â”œâ”€ ä¸»è¦è¯­è¨€: {repo['language']}\n")
        f.write(f"   â”œâ”€ åˆ›å»ºæ—¶é—´: {repo.get('created_at', 'N/A')}\n")
        f.write(f"   â””â”€ Stars: {repo['stars']}, Forks: {repo['forks']}\n")
        
        f.write("\n" + "=" * 80 + "\n\n")
    
    def generate_detailed_json_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„ JSON æŠ¥å‘Š"""
        print("ğŸ“„ ç”Ÿæˆè¯¦ç»† JSON æŠ¥å‘Š...")
        
        results = self.data.get('results', [])
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary = self._calculate_summary(results)
        
        report_data = {
            **self.data,  # åŒ…å«åŸå§‹æ•°æ®
            'summary': summary,
            'report_generated_at': datetime.now().isoformat()
        }
        
        with open(f'{self.reports_dir}/detailed_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
    
    def _calculate_summary(self, results: List[Dict]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡æ‘˜è¦"""
        if not results:
            return {}
        
        # åŸºæœ¬ç»Ÿè®¡
        public_repos = sum(1 for r in results if not r['repository']['private'])
        private_repos = sum(1 for r in results if r['repository']['private'])
        total_matches = sum(r['file']['match_count'] for r in results)
        repositories = list(set(r['repository']['full_name'] for r in results))
        
        # æ—¶é—´ç»Ÿè®¡
        file_ages = [r.get('time_info', {}).get('file_age_days') for r in results]
        file_ages = [age for age in file_ages if age is not None]
        
        oldest_file_days = max(file_ages) if file_ages else 0
        newest_file_days = min(file_ages) if file_ages else 0
        avg_file_age = sum(file_ages) / len(file_ages) if file_ages else 0
        
        # ä½œè€…ç»Ÿè®¡
        authors = set()
        for r in results:
            time_info = r.get('time_info', {})
            first_author = time_info.get('first_commit', {}).get('first_author')
            last_author = time_info.get('last_commit', {}).get('last_author')
            if first_author:
                authors.add(first_author)
            if last_author:
                authors.add(last_author)
        
        # æäº¤ç»Ÿè®¡
        total_commits = sum(r.get('time_info', {}).get('total_commits', 0) for r in results)
        
        # æ–‡ä»¶æ‰©å±•åç»Ÿè®¡
        extensions = {}
        for r in results:
            file_path = r['file']['path']
            ext = file_path.split('.')[-1].lower() if '.' in file_path else 'no_extension'
            extensions[ext] = extensions.get(ext, 0) + 1
        
        # ä»“åº“è¯­è¨€ç»Ÿè®¡
        languages = {}
        for r in results:
            lang = r['repository']['language'] or 'Unknown'
            languages[lang] = languages.get(lang, 0) + 1
        
        return {
            'public_repos': public_repos,
            'private_repos': private_repos,
            'total_matches': total_matches,
            'repositories': repositories,
            'repository_count': len(repositories),
            'oldest_file_days': oldest_file_days,
            'newest_file_days': newest_file_days,
            'avg_file_age_days': round(avg_file_age, 1),
            'total_commits': total_commits,
            'unique_authors': list(authors),
            'author_count': len(authors),
            'file_extensions': extensions,
            'repository_languages': languages,
            'risk_level': self._calculate_risk_level(public_repos, total_matches)
        }
    
    def _calculate_risk_level(self, public_repos: int, total_matches: int) -> str:
        """è®¡ç®—é£é™©ç­‰çº§"""
        if public_repos > 0:
            if total_matches > 10:
                return "CRITICAL"
            elif total_matches > 5:
                return "HIGH"
            else:
                return "MEDIUM"
        elif total_matches > 0:
            return "LOW"
        else:
            return "NONE"
    
    def generate_csv_report(self):
        """ç”Ÿæˆ CSV æ ¼å¼æŠ¥å‘Š"""
        print("ğŸ“„ ç”Ÿæˆ CSV æŠ¥å‘Š...")
        
        results = self.data.get('results', [])
        
        with open(f'{self.reports_dir}/search_results.csv', 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = [
                'repository_name', 'repository_owner', 'repository_private', 'repository_url',
                'file_path', 'file_name', 'file_url', 'file_size', 'match_count',
                'first_created', 'first_author', 'last_modified', 'last_author',
                'file_age_days', 'total_commits', 'last_commit_sha', 'change_status',
                'additions', 'deletions', 'repo_stars', 'repo_forks', 'repo_language'
            ]
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in results:
                repo = result['repository']
                file_info = result['file']
                time_info = result.get('time_info', {})
                first_commit = time_info.get('first_commit', {})
                last_commit = time_info.get('last_commit', {})
                change_info = result.get('change_info', {})
                
                writer.writerow({
                    'repository_name': repo['full_name'],
                    'repository_owner': repo['owner'],
                    'repository_private': repo['private'],
                    'repository_url': repo['html_url'],
                    'file_path': file_info['path'],
                    'file_name': file_info['name'],
                    'file_url': file_info['html_url'],
                    'file_size': file_info['size'],
                    'match_count': file_info['match_count'],
                    'first_created': first_commit.get('first_created', ''),
                    'first_author': first_commit.get('first_author', ''),
                    'last_modified': last_commit.get('last_modified', ''),
                    'last_author': last_commit.get('last_author', ''),
                    'file_age_days': time_info.get('file_age_days', ''),
                    'total_commits': time_info.get('total_commits', 0),
                    'last_commit_sha': last_commit.get('last_commit_sha', ''),
                    'change_status': change_info.get('status', ''),
                    'additions': change_info.get('additions', 0),
                    'deletions': change_info.get('deletions', 0),
                    'repo_stars': repo['stars'],
                    'repo_forks': repo['forks'],
                    'repo_language': repo['language']
                })
    
    def generate_markdown_report(self):
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
        print("ğŸ“„ ç”Ÿæˆ Markdown æŠ¥å‘Š...")
        
        results = self.data.get('results', [])
        summary = self._calculate_summary(results)
        
        with open(f'{self.reports_dir}/search_results.md', 'w', encoding='utf-8') as f:
            f.write("# GitHub æ•æ„Ÿå†…å®¹æœç´¢æŠ¥å‘Š\n\n")
            
            # æ¦‚è§ˆ
            f.write("## ğŸ“Š æ‰«ææ¦‚è§ˆ\n\n")
            f.write(f"- **æ‰«ææ—¶é—´**: {self.data.get('scan_time', 'N/A')}\n")
            f.write(f"- **æœç´¢æ¨¡å¼**: `{self.data.get('search_pattern', 'N/A')}`\n")
            f.write(f"- **æ–‡ä»¶ç±»å‹**: {', '.join(self.data.get('file_extensions', []))}\n")
            f.write(f"- **æœç´¢èŒƒå›´**: {self.data.get('search_scope', 'å…¨éƒ¨å…¬å¼€ä»“åº“')}\n")
            f.write(f"- **æ€»å…±æ‰¾åˆ°**: {self.data.get('total_found', 0)} ä¸ªç»“æœ\n")
            f.write(f"- **å·²åˆ†æ**: {len(results)} ä¸ªæ–‡ä»¶\n\n")
            
            if summary:
                # é£é™©è¯„ä¼°
                risk_level = summary.get('risk_level', 'UNKNOWN')
                risk_emoji = {'CRITICAL': 'ğŸš¨', 'HIGH': 'âš ï¸', 'MEDIUM': 'ğŸŸ¡', 'LOW': 'ğŸŸ¢', 'NONE': 'âœ…'}.get(risk_level, 'â“')
                f.write(f"## {risk_emoji} é£é™©è¯„ä¼°: {risk_level}\n\n")
                
                # ç»Ÿè®¡æ‘˜è¦
                f.write("## ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦\n\n")
                f.write(f"- **å…¬å¼€ä»“åº“æ–‡ä»¶**: {summary.get('public_repos', 0)} ä¸ª\n")
                f.write(f"- **ç§æœ‰ä»“åº“æ–‡ä»¶**: {summary.get('private_repos', 0)} ä¸ª\n")
                f.write(f"- **æ€»åŒ¹é…æ¬¡æ•°**: {summary.get('total_matches', 0)} æ¬¡\n")
                f.write(f"- **æ¶‰åŠä»“åº“æ•°**: {summary.get('repository_count', 0)} ä¸ª\n")
                f.write(f"- **æ¶‰åŠä½œè€…æ•°**: {summary.get('author_count', 0)} äºº\n")
                f.write(f"- **å¹³å‡æ–‡ä»¶å¹´é¾„**: {summary.get('avg_file_age_days', 0)} å¤©\n")
                f.write(f"- **æ€»æäº¤æ¬¡æ•°**: {summary.get('total_commits', 0)} æ¬¡\n\n")
                
                # æ–‡ä»¶æ‰©å±•ååˆ†å¸ƒ
                extensions = summary.get('file_extensions', {})
                if extensions:
                    f.write("### ğŸ“ æ–‡ä»¶ç±»å‹åˆ†å¸ƒ\n\n")
                    for ext, count in sorted(extensions.items(), key=lambda x: x[1], reverse=True):
                        f.write(f"- **{ext}**: {count} ä¸ªæ–‡ä»¶\n")
                    f.write("\n")
                
                # ä»“åº“è¯­è¨€åˆ†å¸ƒ
                languages = summary.get('repository_languages', {})
                if languages:
                    f.write("### ğŸ’» ä»“åº“è¯­è¨€åˆ†å¸ƒ\n\n")
                    for lang, count in sorted(languages.items(), key=lambda x: x[1], reverse=True):
                        f.write(f"- **{lang}**: {count} ä¸ªä»“åº“\n")
                    f.write("\n")
            
            if not results:
                f.write("## âœ… æ‰«æç»“æœ\n\n")
                f.write("æœªå‘ç°åŒ…å«æ•æ„Ÿå†…å®¹çš„æ–‡ä»¶ã€‚\n\n")
                return
            
            # è¯¦ç»†ç»“æœ
            f.write("## ğŸ“‹ è¯¦ç»†å‘ç°\n\n")
            
            for i, result in enumerate(results, 1):
                repo = result['repository']
                file_info = result['file']
                time_info = result.get('time_info', {})
                
                f.write(f"### {i}. {repo['full_name']}\n\n")
                
                # åŸºæœ¬ä¿¡æ¯è¡¨æ ¼
                f.write("| å±æ€§ | å€¼ |\n")
                f.write("|------|----|\n")
                f.write(f"| æ–‡ä»¶è·¯å¾„ | [`{file_info['path']}`]({file_info['html_url']}) |\n")
                f.write(f"| ä»“åº“ç±»å‹ | {'ğŸ”’ ç§æœ‰' if repo['private'] else 'ğŸŒ å…¬å¼€'} |\n")
                f.write(f"| æ–‡ä»¶å¤§å° | {file_info['size']} bytes |\n")
                f.write(f"| åŒ¹é…æ¬¡æ•° | {file_info['match_count']} |\n")
                
                first_commit = time_info.get('first_commit', {})
                last_commit = time_info.get('last_commit', {})
                
                if first_commit:
                    f.write(f"| é¦–æ¬¡åˆ›å»º | {first_commit.get('first_created', 'N/A')[:10]} |\n")
                    f.write(f"| åˆ›å»ºä½œè€… | {first_commit.get('first_author', 'N/A')} |\n")
                
                if last_commit:
                    f.write(f"| æœ€åä¿®æ”¹ | {last_commit.get('last_modified', 'N/A')[:10]} |\n")
                    f.write(f"| ä¿®æ”¹ä½œè€… | {last_commit.get('last_author', 'N/A')} |\n")
                
                file_age = time_info.get('file_age_days')
                if file_age is not None:
                    f.write(f"| æ–‡ä»¶å¹´é¾„ | {file_age} å¤© |\n")
                
                f.write(f"| æ€»æäº¤æ•° | {time_info.get('total_commits', 0)} |\n")
                f.write(f"| Stars | {repo['stars']} |\n")
                f.write(f"| Forks | {repo['forks']} |\n")
                f.write("\n")
                
                # ä»“åº“æè¿°
                if repo.get('description'):
                    f.write(f"**æè¿°**: {repo['description']}\n\n")
                
                f.write("---\n\n")
    
    def generate_summary_report(self):
        """ç”Ÿæˆç®€è¦æ‘˜è¦æŠ¥å‘Š"""
        print("ğŸ“„ ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š...")
        
        results = self.data.get('results', [])
        summary = self._calculate_summary(results)
        
        summary_data = {
            'scan_time': self.data.get('scan_time'),
            'search_pattern': self.data.get('search_pattern'),
            'total_found': self.data.get('total_found', 0),
            'analyzed_files': len(results),
            'summary': summary,
            'critical_findings': self._get_critical_findings(results),
            'recommendations': self._get_recommendations(summary)
        }
        
        with open(f'{self.reports_dir}/summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    def _get_critical_findings(self, results: List[Dict]) -> List[Dict]:
        """è·å–å…³é”®å‘ç°"""
        critical = []
        
        for result in results:
            if not result['repository']['private']:  # å…¬å¼€ä»“åº“
                critical.append({
                    'repository': result['repository']['full_name'],
                    'file': result['file']['path'],
                    'matches': result['file']['match_count'],
                    'url': result['file']['html_url'],
                    'reason': 'Public repository exposure'
                })
        
        # æŒ‰åŒ¹é…æ¬¡æ•°æ’åº
        critical.sort(key=lambda x: x['matches'], reverse=True)
        return critical[:10]  # åªè¿”å›å‰10ä¸ª
    
    def _get_recommendations(self, summary: Dict) -> List[str]:
        """è·å–å®‰å…¨å»ºè®®"""
        recommendations = []
        
        risk_level = summary.get('risk_level', 'NONE')
        public_repos = summary.get('public_repos', 0)
        
        if risk_level in ['CRITICAL', 'HIGH']:
            recommendations.append("ğŸš¨ ç«‹å³æ›´æ¢æ‰€æœ‰ç›¸å…³çš„ API å¯†é’¥")
            recommendations.append("ğŸ“ è”ç³»ç›¸å…³ä»“åº“æ‰€æœ‰è€…åˆ é™¤æ•æ„Ÿå†…å®¹")
            
        if public_repos > 0:
            recommendations.append("ğŸ”’ å°†åŒ…å«æ•æ„Ÿä¿¡æ¯çš„ä»“åº“è®¾ä¸ºç§æœ‰")
            recommendations.append("ğŸ§¹ ä½¿ç”¨ BFG Repo-Cleaner æ¸…ç† Git å†å²")
            
        recommendations.extend([
            "ğŸ›¡ï¸ å¯ç”¨ GitHub Secret Scanning",
            "ğŸ”§ é…ç½® pre-commit hooks é˜²æ­¢æœªæ¥æ³„éœ²",
            "ğŸ“‹ å»ºç«‹æ•æ„Ÿä¿¡æ¯ç®¡ç†æµç¨‹",
            "ğŸ”„ å®šæœŸè¿è¡Œå®‰å…¨æ‰«æ"
        ])
        
        return recommendations


def main():
    """ä¸»å‡½æ•°"""
    try:
        generator = ReportGenerator()
        generator.generate_all_reports()
        
        print(f"\nğŸ“ æŠ¥å‘Šæ–‡ä»¶ä½ç½®:")
        print(f"â”œâ”€â”€ reports/search_results.txt     (æ–‡æœ¬æ ¼å¼)")
        print(f"â”œâ”€â”€ reports/detailed_report.json  (è¯¦ç»† JSON)")
        print(f"â”œâ”€â”€ reports/search_results.csv    (CSV è¡¨æ ¼)")
        print(f"â”œâ”€â”€ reports/search_results.md     (Markdown)")
        print(f"â”œâ”€â”€ reports/summary.json          (æ‘˜è¦)")
        print(f"â””â”€â”€ reports/raw_data.json         (åŸå§‹æ•°æ®)")
        
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        exit(1)


if __name__ == "__main__":
    main()
